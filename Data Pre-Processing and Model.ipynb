{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn import metrics\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import KFold, StratifiedKFold, cross_val_score \n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.over_sampling import SMOTE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"train.csv\")\n",
    "df_test = pd.read_csv(\"test.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Observation from dataset: all 1 valued target rows are in starting of the dataÂ¶\n",
    "so we will shuffle the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.sample(frac=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_1 = np.where(df.m13 == 1)\n",
    "idx_0 = np.where(df.m13 == 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Checking if shuffling was done successfully, first line of code gives indexes of rows containing value =1 in column m13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "ntrain = df.shape[0]\n",
    "ntest = df_test.shape[0]\n",
    "y_train = df.m13.values\n",
    "all_data = pd.concat([df, df_test], sort=False).reset_index(drop=True)\n",
    "all_data.drop(['m13'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['m13'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.isnull().sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data['first_payment_date'].unique()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Will treat it as categorical variable, Also Notice that date format is different in test data and training data, so we need to make it consistent first otherwise it will create unnecessary dummy variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data.replace(\"Apr-12\", \"04/2012\", inplace = True)\n",
    "all_data.replace(\"Mar-12\", \"03/2012\", inplace = True)\n",
    "all_data.replace(\"May-12\", \"05/2012\", inplace = True)\n",
    "all_data.replace(\"Feb-12\", \"02/2012\", inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data['origination_date'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to first payment date we can see different formatting for same date here, so we will convert them here again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data.replace(\"2012-02-01\", \"01/02/12\", inplace = True)\n",
    "all_data.replace(\"2012-01-01\", \"01/01/12\", inplace = True)\n",
    "all_data.replace(\"2012-03-01 \", \"01/03/12 \", inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data['loan_purpose'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data['source'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data['financial_institution'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr = all_data.corr()\n",
    "sns.heatmap(corr,\n",
    "            xticklabels=corr.columns.values,\n",
    "            yticklabels=corr.columns.values)\n",
    "pos_filtered_corr = corr[ corr.iloc[:,:] >= 0.5] # from this we easily know highly positively correlated values\n",
    "neg_filtered_corr = corr[ corr.iloc[:,:] <= -0.5] # no attribute is that highly negatively correlated\n",
    "sns.heatmap(pos_filtered_corr,\n",
    "            xticklabels=pos_filtered_corr.columns.values,\n",
    "            yticklabels=pos_filtered_corr.columns.values) #only of the attributes which are correlated highly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From correlation matrix, we found that following features are highly correlated but removing them is affection our final model in negative ways so we will not remove them:\n",
    "1. Borrower credit sccore and number of borowers- they are connected 99% so I will remove one of them for sure\n",
    "2. m8 and m9\n",
    "3. m9 and m10\n",
    "4. m9 , m10 and m11\n",
    "5. m10, m11 and m12\n",
    "Also, from a general observation origination_date is a redundant attribute if we are usign first_payment_date, so I will remove it as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#all_data = all_data.drop([\"co-borrower_credit_score\", \"m9\", \"m10\", \"m11\", \"m12\"], axis=1) # based on correlation\n",
    "all_data = all_data.drop([\"origination_date\", \"co-borrower_credit_score\"], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data['loan_purpose'] = all_data['loan_purpose'].astype(str)\n",
    "all_data['first_payment_date'] = all_data['first_payment_date'].astype(str)\n",
    "all_data['financial_institution'] = all_data['financial_institution'].astype(str)\n",
    "all_data['origination_date'] = all_data['origination_date'].astype(str)\n",
    "all_data['source'] = all_data['source'].astype(str)\n",
    "Finalall_data = pd.get_dummies(all_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "converting all categorical variables in dummy variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Finalall_data = Finalall_data.drop([\"source_X\", \"financial_institution_OTHER\", \"loan_purpose_A23\", \"first_payment_date_02/2012\", \"origination_date_01/01/12\"], axis=1) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are removing one dummy variable for each categorical variables for those algorithms which are affected by dummy variable trap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = Finalall_data[:ntrain]\n",
    "test = Finalall_data[ntrain:]\n",
    "X = train.drop([\"loan_id\"], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Removed loan id from train data, since it is not useful for training purposes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from sklearn.preprocessing import StandardScaler\n",
    "#scaler = StandardScaler()\n",
    "#X_train = scaler.fit_transform(X_train)\n",
    "#X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Haven't used feature scaler for final model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "class_weights = compute_class_weight('balanced', np.unique(y_train), y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above piece of code gives us the class weights for all possible classes in dataset using 'balanced' technique, same task is performed by Logistic Regression class_weight parameter when its value is chosen to be 'balanced', which is helpful when we are dealing with class imbalance problems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#svm_model = SVC(kernel='rbf', random_state=0, gamma='auto')\n",
    "#svm_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. this is a basic implementation of svm, in one case I took all dummy variables and in another case i took care of dummy variable trap, and the results are 98% same in both cases.\n",
    "2. Apart from that SVM is taking long times for fitting and giving very low f1 value for class '1' (0.07), average macro is fair though(0.53), refer classification report for more details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Classifier = LogisticRegression(random_state = 0, solver='sag', n_jobs=-1, class_weight='balanced', fit_intercept=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observations -\n",
    "1. without fit_intercept=False and class weighs balanced, LR is performing well with avg, f1 value = 0.65, but with them in consideration f1 value is deteriorating to 0.38, weird\n",
    "2. For more details on above point, refer snapshots of classification report\n",
    "3. Also overfitting is observed in this SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Classifier = DecisionTreeClassifier(random_state=0, class_weight='balanced')\n",
    "#Classifier.fit(X_train, y_train)\n",
    "#scv = StratifiedKFold(n_splits=5)\n",
    "#crossvalscore = cross_val_score(estimator=Classifier, X=X_train, y=y_train, cv=scv,  scoring = 'f1_macro')\n",
    "#crossvalscore.mean()\n",
    "#crossvalscore.std()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. In its most basic form, decision tree is performing good, since it have avg. f1 score of around 0.588, though have acceptable value of f1 for class '1' = 0.18, It is performing fast as well.\n",
    "2. When we remove one of dummy varibles for each categorical variable, avg f1 score reduces to 0.53.\n",
    "3. Watch classification report for more details\n",
    "4. DT overfits on training set with macro f1 value = 1\n",
    "5. adding class weight parameter doesn't changes above observations much"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NB = GaussianNB()\n",
    "#NB.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Till now it's been best when training but, during kfold validation mean of f1 score was 0.4 and std = 0.209\n",
    "2. No issues of overfitting\n",
    "3. avg f1 value = 0.62 with acceptable value for f1 of class '1', refer classification report for more\n",
    "4. After removing dummy variable same effects are observed as for DT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#RFC = RandomForestClassifier(random_state=0, class_weight='balanced', n_jobs=-1)\n",
    "#RFC.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Performs pretty much same as DT, maybe a little better, overfitting training data\n",
    "- Refer classification results for more details\n",
    "- I have observed above in all models that kfold gave results similar to shown in classification report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that I have used pretty much everything that I read for imbalanced classes, It's time to try last thing, sampling the data, 1. Oversampling using SMOTE\n",
    "2. Random Under sampling with replacement\n",
    "- After Several Experiments I deduced that oversampling using SMOTE give overall better results than under sampling using RFC alongside, and by overall better results I mean better f1 score for class '1', better overall f1 score on X_test, better accuracy, and overfitting of training data as well.\n",
    "- Also cross val score mean value turns out to be 0.99(again signifies overfitting) when x=X_res and y=y_res, but it is 0.72 for X_train and Y_train\n",
    "Final Conclusion GBC classifier along with sampling is not overfitting that much so tuning it should give better results for future test predicitons as well, so I will further proceed with GBC\n",
    "Note: All above results in this markdown are considerable only when no feature was dropped from original dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#oversampler = SMOTE(sampling_strategy=0.05, random_state=0, k_neighbors=10)\n",
    "#X_res, y_res = oversampler.fit_resample(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So here we have done the sampling, we may want to shuffle the obtained dataset X_res and y_res, though minute changes in results are obtained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_res = pd.DataFrame(X_res)\n",
    "#y_res = pd.DataFrame(y_res)\n",
    "#df_all_rows = pd.concat([X_res, y_res], axis=1)\n",
    "#df_all_rows = df_all_rows.sample(frac=1).reset_index(drop=True)\n",
    "#X_res = df_all_rows.iloc[:,:-1].values\n",
    "#y_res = df_all_rows[0]\n",
    "#y_res = y_res.T.reset_index(drop=True).T\n",
    "#y_res = df_all_rows.iloc[:,-1].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#GBC = GradientBoostingClassifier(random_state=0)\n",
    "#GBC.fit(X_res, y_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#scv = StratifiedKFold(n_splits=5)\n",
    "#crossvalscore = cross_val_score(estimator=GBC, X=X_res, y=y_res, cv=scv,  scoring = 'f1_macro')\n",
    "#crossvalscore.mean()\n",
    "#crossvalscore.std()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the cross-validation step and we have discussed results of it in one of the above markdowns when X and y are changed in this. Note that we have used stratifiedK fold as one of the step for tackling imbalanced classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_grid = [{'min_samples_leaf': [2, 5, 10, 15, 20], 'min_samples_split': [2, 3, 4, 5, 6, 7, 10, 100]\n",
    "                    , 'max_depth': [1, 4, 8, 16 ,32]\n",
    "                    , 'criterion': ['gini']}\n",
    "                    , {'criterion': ['entropy'], 'min_samples_leaf': [2, 5, 10, 15, 20]\n",
    "                    , 'min_samples_split' : [2, 3, 4, 5, 6, 7, 10, 100]\n",
    "                    , 'max_depth': [1, 4, 8, 16 ,32]}]\n",
    "#params_grid = [{'min_samples_leaf': [0.1, 0.5, 5], 'min_samples_split': [2, 3, 4, 5, 6, 7, 10]\n",
    "#                    , 'max_depth': [1, 4, 8, 16 ,32]\n",
    "#                    , 'criterion': ['gini']}\n",
    "#                    , {'criterion': ['entropy'], 'min_samples_leaf': [0.1, 0.5, 5]\n",
    "#                    , 'min_samples_split' : [0.1, 2, 3 , 4, 5, 6, 8, 10]\n",
    "#                    , 'max_depth': [1, 4, 8, 16 ,32]}]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using grid search for hyperparameter tuning in decision tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gridsearch = GridSearchCV(DecisionTreeClassifier(random_state=0, class_weight='balanced'\n",
    "                                                 , presort=True), params_grid\n",
    "                                                , cv= scv, scoring='f1_macro', n_jobs=-1, verbose=50)\n",
    "gridsearch.fit(X_res, y_res)\n",
    "gridsearch.best_params_\n",
    "gridsearch.best_score_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicting on splitted test set and training set to see if overfitting is there or not\n",
    "y_pred_train = gridsearch.best_estimator_.predict(X_train)\n",
    "y_pred = gridsearch.best_estimator_.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "MacroF1_test = f1_score(y_test, y_pred, average='macro')\n",
    "MacroF1_train = f1_score(y_train, y_pred_train, average='macro')\n",
    "\n",
    "# --classification report --\n",
    "Report = metrics.classification_report(y_test, y_pred, labels=[0,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Prediciting on test set\n",
    "y_test_preds = grid_search.best_estimator_.predict(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Preparing to write a CSV file\n",
    "index = pd.DataFrame(test['loan_id'])\n",
    "test = test.drop([\"loan_id\"], axis=1)\n",
    "submission_format = pd.DataFrame(y_test_preds)\n",
    "FinalSubmission = pd.concat([test, submission_format])\n",
    "FinalSubmission = FinalSubmission.sort_values(by='ID')\n",
    "FinalSubmission.to_csv(\"/home/suraj/Desktop/ML Problems/ML hackathon Problem/FinalSubmission.csv\", header=True, index=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "0.21 test score with naive implementation of DT, 0.30 with grid search on decision tree, still need to find reason for such difference in test and cross scores, sampling is also left \n",
    "See what changes can you make in origination data and first payment date variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay I have tried three things-\n",
    "1. first normal original DT parametrized model with 30....\n",
    "2. DT2_parametrized got same result but removed origination date and co-borrower parameter\n",
    "3. DT_parametrized2( removed origination date and co-borrower parameter and have different set of param_grid) and resuls are not at all different"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
